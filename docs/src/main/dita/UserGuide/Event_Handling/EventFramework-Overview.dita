<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_cph_5h4_lx">
    <title>Dataflow Triggers Overview</title>
    <conbody>
        <p><indexterm>dataflow triggers<indexterm>overview</indexterm></indexterm><indexterm>event
                    framework<indexterm>overview</indexterm></indexterm><term>Dataflow
                triggers</term> are instructions for the pipeline to kick off tasks in response to
            events that occur in the pipeline. For example, you can use dataflow triggers to start a
            MapReduce job after the pipeline writes a file to HDFS. Or you might use a dataflow
            trigger to stop a pipeline after the JDBC Query Consumer origin processes all available
            data. </p>
        <p>Configure dataflow triggers with the event framework. You can also use the event
            framework to store event information, such as when an origin starts or completes reading
            a file. </p>
        <p>To use the event framework, first enable event generation for a stage in the pipeline,
            then configure the rest of the event stream to do your bidding. You can add an event
            stream to any pipeline that includes an event-generating stage. </p>
        <p>Event streams consist of the following conceptual components:<dl>
                <dlentry>
                    <dt>event generation</dt>
                    <dd>Events are generated by an event-generating stage when a specific action
                        takes place. The action that generates an event is related to how the stage
                        processes data and differs from stage to stage. </dd>
                    <dd>For example, the Hive Metastore destination updates the Hive metastore, so
                        it generates events each time it changes the metastore. In contrast, the
                        Hadoop FS destination writes files to HDFS, so it generates events each time
                        it closes a file. </dd>
                    <dd>When an event occurs, a stage generates an <term>event record</term> that
                        passes to the pipeline through an event output stream. Event streams cannot
                        be merged with data streams.</dd>
                </dlentry>
                <dlentry>
                    <dt>task execution</dt>
                    <dd>To trigger a task, connect an event stream to an <term>executor</term>.
                        Executor stages perform tasks in <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> or external systems.</dd>
                    <dd>Each time an executor receives an event record, it performs the specified
                        task.</dd>
                    <dd>For example, the Hive Query executor runs user-defined Hive or Impala
                        queries each time it receives an event, and the MapReduce executor triggers
                        a MapReduce job when it receives events. Within <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        />, the Pipeline Finisher executor stops a pipeline upon receiving an event,
                        transitioning the pipeline to a Finished state. </dd>
                </dlentry>
                <dlentry>
                    <dt>event storage</dt>
                    <dd>To store event information, connect the event stream to a destination. The
                        destination writes the event records to the destination system, just like
                        any other data.</dd>
                    <dd>For example, you might store event records to keep an audit trail of the
                        files that the pipeline origin reads. </dd>
                </dlentry>
            </dl></p>
    </conbody>
</concept>

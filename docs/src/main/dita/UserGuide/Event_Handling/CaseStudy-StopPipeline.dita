<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_kff_ykv_lz">
 <title>Case Study: Stop the Pipeline</title>
 <conbody>
  <p><indexterm>event framework case studies<indexterm>stop the pipeline</indexterm></indexterm>Say
            your dataflow topology updates a database table daily at 4 am. Rather than have the
            pipeline process the data in a few minutes and sit idle for the rest of the day, you
            want to kick off the pipeline, have it process all data and then stop - just like old
            school batch processing. </p>
        <p>To do this, simply route the no-more-data event record to the Pipeline Finisher executor. </p>
        <p>The JDBC Query Consumer, JDBC Multitable Consumer, and Salesforce origins all generate
            the no-more-data event when they finish processing queried data. We'll use the JDBC
            Query Consumer to show a more complex scenario. </p>
        <p>Here's the basic pipeline that reads from a database, performs some processing, and
            writes to HDFS: </p>
        <p><image href="../Graphics/Event-StopPipe-Basic.png" id="image_zy1_kkw_lz" scale="60"/></p>
        <p>To configure the pipeline to stop after processing all available queried data:<ol
                id="ol_ivb_qmw_lz">
                <li>Configure the origin to generate events:<p>On the <wintitle>General</wintitle>
                        tab of the JDBC Query Consumer origin, select the <uicontrol>Produce
                            Events</uicontrol> property. </p><p>The event output stream becomes
                        available:</p><p><image href="../Graphics/Event-StopPipe-Event.png"
                            id="image_b5v_znw_lz" scale="55"/></p><p>The JDBC Query Consumer
                        generates several types of events: query success, query failure, and
                        no-more-data. The query success and failure events can be useful, so you
                        might use a Stream Selector to route those records to a separate event
                        stream.</p><p>But let's say we don't care about those events, we just want
                        the no-more-data event to pass to the Pipeline Finisher executor. </p></li>
                <li>Connect the event output stream to the Pipeline Finisher executor. <p>At this
                        point, all events that the origin generates come to the executor. Since the
                        JDBC Query Consumer origin generates multiple event types, this setup might
                        cause the executor to stop the pipeline too soon.</p></li>
                <li>To ensure that only the no-more-data event enters the executor, configure a
                        precondition.<p>With a precondition, only records that meet the specified
                        condition can enter the stage. </p><p>We know that each event record
                        includes the event type in the sdc.event.type record header attribute. So to
                        ensure that only no-more-data events enter the stage, we can use the
                        following expression in the
                    precondition:</p><codeblock>${record:eventType() == 'no-more-data'}</codeblock></li>
                <li>Records that don't meet the precondition go to the stage for error handling, so
                    to avoid storing error records that we don't care about – that is, the query
                    success and failure events – let's also set the <uicontrol>On Record
                        Error</uicontrol> property to <uicontrol>Discard</uicontrol>.<p>There's
                        nothing else to configure in the Pipeline Finisher executor, so here's the
                        final pipeline:</p><p><image href="../Graphics/Event-StopPipe-Finisher.png"
                            id="image_ucl_4qw_lz" scale="65"/></p></li>
            </ol>Congratulations, you've configured a dataflow trigger! With this setup, the JDBC
            Query Consumer passes a no-more-data event when it completes processing all data
            returned by the query, and the Pipeline Finisher executor stops the pipeline and
            transitions the pipeline to a Finished state. All other events generated by the origin
            are discarded. And the next time you want to process more data, you just start the
            pipeline again. </p>
 </conbody>
</concept>

<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_z3m_v52_zq">
 <title>Record Deduplicator</title>
 <shortdesc>The Record Deduplicator evaluates records for duplicate data and routes data to two
        streams -  one for unique records and one for duplicate records. Use the Record Deduplicator
        to discard duplicate data or route duplicate data through different processing logic. </shortdesc>
 <conbody>
  <p><indexterm>Record Deduplicator
                    processor<indexterm>overview</indexterm></indexterm><indexterm>processors<indexterm>Record
                    Deduplicator</indexterm></indexterm>The Record Deduplicator can compare entire
            records or a subset of fields. Use a subset of fields to focus the comparison on fields
            of concern. For example, to discard purchases that are accidentally submitted more than
            once, you might compare information about the purchaser, selected items, and shipping
            address, but ignore the timestamp of the event. </p>
        <p>To enhance pipeline performance, the Record Deduplicator hashes comparison fields and
            uses the hashed values to evaluate for duplicates. On rare occasions, hash functions can
            generate collisions that can cause records to be incorrectly treated as duplicates.</p>
 </conbody>
</concept>

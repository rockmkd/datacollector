<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_bsw_cky_11b">
 <title>What's New in 2.6.0.0</title>
 <conbody>
  <p><indexterm>what's new<indexterm>version 2.6.0.0</indexterm></indexterm><ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            version 2.6.0.0 includes the following new features and enhancements:<dl>
                <dlentry>
                    <dt>Installation</dt>
                    <dd>
                        <ul id="ul_s2s_lky_11b">
                            <li>MapR prerequisites - You can now run the <codeph>setup-mapr</codeph>
                                command in interactive or non-interactive mode. In interactive mode,
                                the command prompts you for the MapR version and home directory. In
                                non-interactive mode, you define the MapR version and home directory
                                in environment variables before running the command.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Stage Libraries</dt>
                    <dd>Data Collector now supports the following stage libraries:<ul
                            id="ul_zcv_4ky_11b">
                            <li dir="ltr">
                                <p dir="ltr">Hortonworks version 2.6 distribution of Apache
                                    Hadoop</p>
                            </li>
                        </ul></dd>
                </dlentry>
                <dlentry>
                    <dt>Drift Synchronization Solution for Hive</dt>
                    <dd><ul id="ul_x5p_zky_11b">
                            <li><xref
                                    href="../Hive_Drift_Solution/ParquetProcessing.dita#concept_ndg_3zw_vz"
                                    >Parquet support</xref> - You can now use the <xref
                                    href="../Hive_Drift_Solution/HiveDrift-Overview.dita#concept_phk_bdf_2w"
                                    >Drift Synchronization for Hive</xref> to generate Parquet
                                files. Previously, the Data Synchronization Solution supported only
                                Avro data. This enhancement includes the following updates:<ul
                                    id="ul_dt1_sky_11b">
                                    <li><xref
                                            href="../Processors/HiveMetadata.dita#concept_rz5_nft_zv"
                                            >Hive Metadata processor</xref> data format property -
                                        Use the new data format property to indicate the data format
                                        to use.</li>
                                    <li>Parquet support in the <xref
                                            href="../Destinations/HiveMetastore.dita#concept_gcr_z2t_zv"
                                            >Hive Metastore destination</xref> - The destination can
                                        now create and update Parquet tables in Hive. The
                                        destination no longer includes a data format property since
                                        that information is now configured in the Hive Metadata
                                        processor. </li>
                                </ul></li>
                        </ul>See the documentation <xref
                            href="../Hive_Drift_Solution/Implementation.dita#concept_y5w_dj3_fw"
                            >implementation details</xref> and a <xref
                            href="../Hive_Drift_Solution/CaseStudy-Parquet.dita#concept_vl3_v2f_zz"
                            >Parquet case study</xref>. </dd>
                </dlentry>
                <dlentry>
                    <dt>Multithreaded Pipelines</dt>
                    <dd>The <xref
                            href="../Multithreaded_Pipelines/MultithreadedPipelines_Overview.dita#concept_zpp_2xc_py"
                            >multithreaded framework</xref> includes the following enhancements:<ul
                            id="ul_a2d_4ry_11b">
                            <li>Origins for multithreaded pipelines - You can now use the following
                                origins to create multithreaded pipelines:<ul id="ul_efl_4ry_11b">
                                    <li dir="ltr">
                                        <p dir="ltr"><xref
                                                href="../Origins/CoAPServer.dita#concept_wfy_ghn_sz"
                                                >CoAP Server origin</xref></p>
                                    </li>
                                    <li dir="ltr">
                                        <p dir="ltr"><xref
                                                href="../Origins/TCPServer.dita#concept_ppm_xb1_4z"
                                                >TCP Server origin</xref></p>
                                    </li>
                                </ul></li>
                            <li>Multithreaded origin icons - The icons for multithreaded origins now
                                include a multithreaded indicator.</li>
                        </ul></dd>
                </dlentry>
                <dlentry>
                    <dt>Dataflow Triggers / Event Framework</dt>
                    <dd>
                        <ul id="ul_ot4_ssy_11b">
                            <li><xref
                                    href="../Event_Handling/ExecutorStages.dita#concept_rxg_shn_lx"
                                    >New executors</xref> - You can now use the following executors
                                to perform tasks upon receiving an event:<ul id="ul_pd2_tsy_11b">
                                    <li><xref href="../Executors/Email.dita#concept_sjs_sfp_qz"
                                            >Email executor</xref></li>
                                    <li><xref href="../Executors/Shell.dita#concept_jsr_zpw_tz"
                                            >Shell executor</xref>
                                    </li>
                                </ul></li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Origins</dt>
                    <dd>
                        <ul id="ul_xb4_bty_11b">
                            <li><xref href="../Origins/CoAPServer.dita#concept_wfy_ghn_sz">New CoAP
                                    Server origin</xref> - An origin that listens on a CoAP endpoint
                                and processes the contents of all authorized CoAP requests. The
                                origin performs parallel processing and can generate multithreaded
                                pipelines. </li>
                            <li><xref href="../Origins/TCPServer.dita#concept_ppm_xb1_4z">New TCP
                                    Server origin</xref> - An origin that listens at the specified
                                ports, establishes TCP sessions with clients that initiate TCP
                                connections, and then processes the incoming data. The origin can
                                process NetFlow, syslog, and most Data Collector data formats as
                                separated records.</li>
                            <li>SFTP/FTP Client origin enhancement - You can now specify the first
                                file to process. This enables you to skip processing files with
                                earlier timestamps. </li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Processors</dt>
                    <dd>
                        <ul id="ul_dvb_jty_11b">
                            <li>Groovy, JavaScript, and Jython Evaluator processors enhancements:<ul
                                    id="ul_e4s_jty_11b">
                                    <li>You can now include some methods of the sdcFunctions
                                        scripting object in the initialization and destroy scripts
                                        for the processors.</li>
                                    <li>You can now access pipeline constants through scripts.</li>
                                </ul></li>
                            <li>Hive Metadata processor enhancements:<ul id="ul_v3b_lty_11b">
                                    <li>The Hive Metadata processor can now <xref
                                            href="../Hive_Drift_Solution/ParquetProcessing.dita#concept_ndg_3zw_vz"
                                            >process Parquet data as part of the Drift
                                            Synchronization Solution for Hive</xref>. </li>
                                    <li>You can now specify the data format to use: Avro or
                                        Parquet.</li>
                                    <li>You can now configure an expression that defines comments
                                        for generated columns. </li>
                                </ul></li>
                            <li>JDBC Lookup processor enhancements:<ul id="ul_vls_4ty_11b">
                                    <li>The JDBC Lookup processor can now return multiple values.
                                        You can now configure the lookup to return the first value
                                        or to return all matches as separate records. </li>
                                    <li>When you monitor a pipeline that includes the JDBC Lookup
                                        processor, you can now view stage statistics about the
                                        number of queries the processor makes and the average time
                                        of the queries.</li>
                                </ul></li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Destinations</dt>
                    <dd>
                        <ul id="ul_p5x_qty_11b">
                            <li><xref href="../Destinations/CoAPClient.dita#concept_hw5_s3n_sz">New
                                    CoAP Client destination</xref> - A destination that writes to a
                                CoAP endpoint.</li>
                            <li>Hive Metastore destination enhancements:<ul id="ul_y5w_sty_11b">
                                    <li>The destination can now <xref
                                            href="../Destinations/HiveMetastore-TableGen.dita#concept_wyr_5jv_hw"
                                            >create and update Parquet tables in Hive</xref>. </li>
                                    <li>Also, the data format property has been removed. <xref
                                            href="../Processors/HiveMetadata.dita#concept_rz5_nft_zv"
                                            >You now specify the data format in the Hive Metadata
                                            processor.</xref> Since the Hive Metastore previously
                                        supported only Avro data, there is no upgrade impact. </li>
                                    <li dir="ltr">
                                        <p>Kudu destination enhancement - You can use the new
                                            Mutation Buffer Space property to set the buffer size
                                            that the Kudu client uses to write each batch.</p>
                                    </li>
                                </ul></li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Executors</dt>
                    <dd>
                        <ul id="ul_xl4_b5y_11b">
                            <li><xref href="../Executors/Email.dita#concept_sjs_sfp_qz">New Email
                                    executor</xref> - Use to send custom emails upon receiving an
                                event. For a case study, see <xref
                                    href="../Event_Handling/CaseStudy-SendEmail.dita#concept_t2t_lp5_xz"
                                    />.<ul id="ul_ekw_b5y_11b">
                                    <li dir="ltr">
                                        <p><xref href="../Executors/Shell.dita#concept_jsr_zpw_tz"
                                                >New Shell executor</xref> - Use to execute shell
                                            scripts upon receiving an event. </p>
                                    </li>
                                    <li dir="ltr">
                                        <p><xref
                                                href="../Executors/JDBCQuery.dita#concept_j3r_gcv_sx"
                                                >JDBC Query executor enhancement</xref> - A new
                                            Batch Commit property allows the executor to commit to
                                            the database after each batch. Previously, the executor
                                            did not call commits by default.</p>
                                        <p>For new pipelines, the property is enabled by default.
                                            For upgraded pipelines, the property is disabled to
                                            prevent changes in pipeline behavior. </p>
                                    </li>
                                    <li dir="ltr">Spark executor enhancement - The executor now
                                        supports Spark 2.0. </li>
                                </ul></li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>REST API / Command Line Interface</dt>
                    <dd>
                        <ul id="ul_ktv_k5y_11b">
                            <li>Offset management - Both the REST API and command line interface can
                                now retrieve the last-saved offset for a pipeline and set the offset
                                for a pipeline when it is not running. </li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Expression Language</dt>
                    <dd>
                        <ul id="ul_fm5_m5y_11b">
                            <li>New batchSize variable - The batchSize variable returns the number
                                of records included in a batch. Use in <xref
                                    href="../Origins/TCPServer-Acks-Expressions.dita#concept_xgd_zr5_zz"
                                    >TCP Server origin custom acknowledgement messages</xref>. </li>
                            <li>vault:read enhancement - The vault:read function now supports
                                returning the value for a key nested in a map.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>General Enhancements</dt>
                    <dd>
                        <ul id="ul_rlz_55y_11b">
                            <li>Support bundles - You can now use Data Collector to generate a
                                support bundle. A support bundle is a ZIP file that includes Data
                                Collector logs, environment and configuration information, pipeline
                                JSON files, resource files, and pipeline snapshots. <p>You upload
                                    the generated file to the StreamSets support team so that we can
                                    use the information to troubleshoot your support
                                tickets.</p></li>
                            <li>
                                <p>TLS property enhancements - Stages that support SSL/TLS now
                                    provide the following enhanced set of properties that enable
                                    more specific configuration:<ul id="ul_v5g_w5y_11b">
                                        <li>Keystore and truststore type - You can now choose
                                            between Java Keystore (JKS) and PKCS-12 (p-12).
                                            Previously, Data Collector always used JKS.</li>
                                        <li>Transport protocols- You can now specify the transport
                                            protocols that you want to allow. By default, Data
                                            Collector uses TLSv1.2. </li>
                                        <li>Cipher suites - You can now specify the cipher suite to
                                            use. Data Collector provides a set of default cipher
                                            suites. Previously, Data Collector always used the
                                            default cipher suites.</li>
                                    </ul>To avoid upgrade impact, all SSL/TLS/HTTPS properties in
                                    existing pipelines are preserved during upgrade. </p>
                            </li>
                            <li>
                                <p>Cluster pipeline enhancement - Cluster streaming mode now
                                    supports Spark 2.0.</p>
                            </li>
                        </ul>
                    </dd>
                </dlentry>
            </dl></p>
 </conbody>
</concept>

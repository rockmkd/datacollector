<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_bsw_cky_11b">
 <title>What's New in 2.6.0.0</title>
 <conbody>
  <p><indexterm>what's new<indexterm>version 2.6.0.0</indexterm></indexterm><ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            version 2.6.0.0 includes the following new features and enhancements:<dl>
                <dlentry>
                    <dt>Installation</dt>
                    <dd>
                        <ul id="ul_s2s_lky_11b">
                            <li><xref
                                    href="../Installation/MapR-Prerequisites.dita#concept_jgs_qpg_2v"
                                    >MapR prerequisites</xref> - You can now run the
                                    <codeph>setup-mapr</codeph> command in interactive or
                                non-interactive mode. In interactive mode, the command prompts you
                                for the MapR version and home directory. In non-interactive mode,
                                you define the MapR version and home directory in environment
                                variables before running the command.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Stage Libraries</dt>
                    <dd>Data Collector now supports the following <xref
                            href="../Installation/AvailableStageLibraries.dita#concept_evs_xkm_s5"
                            >stage libraries</xref>:<ul id="ul_zcv_4ky_11b">
                            <li dir="ltr">
                                <p dir="ltr">Hortonworks version 2.6 distribution of Apache
                                    Hadoop</p>
                            </li>
                            <li>Cloudera distribution of Spark 2.1</li>
                            <li>MapR distribution of Spark 2.1</li>
                        </ul></dd>
                </dlentry>
                <dlentry>
                    <dt>Data Collector Configuration</dt>
                    <dd>
                        <ul id="ul_jys_cpm_d1b">
                            <li><xref
                                    href="../Configuration/ConfiguringDataCollector.dita#task_lxk_kjw_1r"
                                    >New buffer size configuration</xref> - You can now use a new
                                parser.limit configuration property to increase the Data Collector
                                parser buffer size. The parser buffer is used by the origin to
                                process many data formats, including Delimited, JSON, and XML. The
                                parser buffer size limits the size of the records that origins can
                                process. The Data Collector parser buffer size is 1048576 bytes by
                                default.</li>
                        </ul>
                    </dd>
                </dlentry>
            </dl><dl>
                <dlentry>
                    <dt>Drift Synchronization Solution for Hive</dt>
                    <dd><ul id="ul_x5p_zky_11b">
                            <li><xref
                                    href="../Hive_Drift_Solution/ParquetProcessing.dita#concept_ndg_3zw_vz"
                                    >Parquet support</xref> - You can now use the <xref
                                    href="../Hive_Drift_Solution/HiveDrift-Overview.dita#concept_phk_bdf_2w"
                                    >Drift Synchronization Solution for Hive</xref> to generate
                                Parquet files. Previously, the Data Synchronization Solution
                                supported only Avro data. This enhancement includes the following
                                    updates:<ul id="ul_dt1_sky_11b">
                                    <li><xref
                                            href="../Processors/HiveMetadata.dita#concept_rz5_nft_zv"
                                            >Hive Metadata processor</xref> data format property -
                                        Use the new data format property to indicate the data format
                                        to use.</li>
                                    <li>Parquet support in the <xref
                                            href="../Destinations/HiveMetastore.dita#concept_gcr_z2t_zv"
                                            >Hive Metastore destination</xref> - The destination can
                                        now create and update Parquet tables in Hive. The
                                        destination no longer includes a data format property since
                                        that information is now configured in the Hive Metadata
                                        processor. </li>
                                </ul></li>
                        </ul>See the documentation for implementation details and a <xref
                            href="../Hive_Drift_Solution/CaseStudy-Parquet.dita#concept_vl3_v2f_zz"
                            >Parquet case study</xref>. </dd>
                </dlentry>
                <dlentry>
                    <dt>Multithreaded Pipelines</dt>
                    <dd>The <xref
                            href="../Multithreaded_Pipelines/MultithreadedPipelines_Overview.dita#concept_zpp_2xc_py"
                            >multithreaded framework</xref> includes the following enhancements:<ul
                            id="ul_a2d_4ry_11b">
                            <li><xref
                                    href="../Multithreaded_Pipelines/Origins.dita#concept_wcz_tpd_py"
                                    >Origins for multithreaded pipelines</xref> - You can now use
                                the following origins to create multithreaded pipelines:<ul
                                    id="ul_efl_4ry_11b">
                                    <li dir="ltr">
                                        <p dir="ltr"><xref
                                                href="../Origins/CoAPServer.dita#concept_wfy_ghn_sz"
                                                >CoAP Server origin</xref></p>
                                    </li>
                                    <li dir="ltr">
                                        <p dir="ltr"><xref
                                                href="../Origins/TCPServer.dita#concept_ppm_xb1_4z"
                                                >TCP Server origin</xref></p>
                                    </li>
                                </ul></li>
                            <li>Multithreaded origin icons - The icons for multithreaded origins now
                                include the following multithreaded indicator: <image
                                    href="../Graphics/icon_Multithreaded.png" id="image_al4_tpm_d1b"
                                    scale="30"/><p>For example, hereâ€™s the updated Elasticsearch
                                    origin icon:</p><p><image
                                        href="../Graphics/Multithreaded-icon.png"
                                        id="image_iml_ypm_d1b" scale="30"/></p></li>
                        </ul></dd>
                </dlentry>
                <dlentry>
                    <dt>Dataflow Triggers / Event Framework</dt>
                    <dd>
                        <ul id="ul_ot4_ssy_11b">
                            <li><xref
                                    href="../Event_Handling/ExecutorStages.dita#concept_rxg_shn_lx"
                                    >New executors</xref> - You can now use the following executors
                                to perform tasks upon receiving an event:<ul id="ul_pd2_tsy_11b">
                                    <li><xref href="../Executors/Email.dita#concept_sjs_sfp_qz"
                                            >Email executor</xref></li>
                                    <li><xref href="../Executors/Shell.dita#concept_jsr_zpw_tz"
                                            >Shell executor</xref>
                                    </li>
                                </ul></li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Dataflow Performance Manager (DPM)</dt>
                    <dd>
                        <ul id="ul_pht_hld_d1b">
                            <li><xref href="../DPM/AggregatedStatistics.dita#concept_h2q_mb5_xw"
                                    >Pipeline statistics</xref> - You can now configure a pipeline
                                to <xref
                                    href="../DPM/AggStatistics_DirectlyToDPM.dita#concept_abc_1w1_c1b"
                                    >write statistics directly to DPM</xref>. Write statistics
                                directly to DPM when you run a job for the pipeline on a single Data
                                Collector. <p>When you run a job on multiple Data Collectors, a
                                    remote pipeline instance runs on each of the Data Collectors. To
                                    view aggregated statistics for the job within DPM, you must
                                    configure the pipeline to write the statistics to a Kafka
                                    cluster, Amazon Kinesis Streams, or SDC RPC.</p></li>
                            <li><xref href="../DPM/PublishPipelines.dita#task_rxy_xqc_fx">Update
                                    published pipelines</xref> - When you update a published
                                pipeline, Data Collector now displays a red asterisk next to the
                                pipeline name to indicate that the pipeline has been updated since
                                it was last published.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Origins</dt>
                    <dd>
                        <ul id="ul_xb4_bty_11b">
                            <li><xref href="../Origins/CoAPServer.dita#concept_wfy_ghn_sz">New CoAP
                                    Server origin</xref> - An origin that listens on a CoAP endpoint
                                and processes the contents of all authorized CoAP requests. The
                                origin performs parallel processing and can generate multithreaded
                                pipelines. </li>
                            <li><xref href="../Origins/TCPServer.dita#concept_ppm_xb1_4z">New TCP
                                    Server origin</xref> - An origin that listens at the specified
                                ports, establishes TCP sessions with clients that initiate TCP
                                connections, and then processes the incoming data. The origin can
                                process NetFlow, syslog, and most Data Collector data formats as
                                separated records. You can configure custom acknowledgement messages
                                and use a new batchSize variable, as well as other expressions, in
                                the messages.</li>
                            <li><xref href="../Origins/SFTP-FirstFile.dita#concept_g5p_5ks_b1b"
                                    >SFTP/FTP Client origin enhancement</xref> - You can now specify
                                the first file to process. This enables you to skip processing files
                                with earlier timestamps. </li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Processors</dt>
                    <dd>
                        <ul id="ul_dvb_jty_11b">
                            <li><xref href="../Processors/Groovy.dita#concept_ldh_sct_gv"
                                    >Groovy</xref>, <xref
                                    href="../Processors/JavaScript.dita#concept_n2p_jgf_lr"
                                    >JavaScript</xref>, and <xref
                                    href="../Processors/Jython.dita#concept_a1h_lkf_lr">Jython
                                    Evaluator</xref> processor enhancements:<ul id="ul_e4s_jty_11b">
                                    <li>You can now include some methods of the sdcFunctions
                                        scripting object in the initialization and destroy scripts
                                        for the processors.</li>
                                    <li>You can now use runtime parameters in the code developed for
                                        a Groovy Evaluator processor.</li>
                                </ul></li>
                            <li><xref href="../Processors/HiveMetadata.dita#concept_rz5_nft_zv">Hive
                                    Metadata processor enhancements</xref>:<ul id="ul_v3b_lty_11b">
                                    <li>The Hive Metadata processor can now <xref
                                            href="../Hive_Drift_Solution/ParquetProcessing.dita#concept_ndg_3zw_vz"
                                            >process Parquet data as part of the Drift
                                            Synchronization Solution for Hive</xref>. </li>
                                    <li>You can now specify the data format to use: Avro or
                                        Parquet.</li>
                                    <li>You can now configure an expression that defines comments
                                        for generated columns. </li>
                                </ul></li>
                            <li><xref href="../Processors/JDBCLookup.dita#concept_ysc_ccy_hw">JDBC
                                    Lookup processor enhancements</xref>:<ul id="ul_vls_4ty_11b">
                                    <li>The JDBC Lookup processor can now return multiple values.
                                        You can now configure the lookup to return the first value
                                        or to return all matches as separate records. </li>
                                    <li>When you <xref
                                            href="../Processors/JDBCLookup-Monitoring.dita#concept_k3l_hrd_wz"
                                            >monitor a pipeline that includes the JDBC Lookup
                                            processor</xref>, you can now view stage statistics
                                        about the number of queries the processor makes and the
                                        average time of the queries.</li>
                                </ul></li>
                            <li><xref href="../Processors/Spark-Versions.dita#concept_nbj_1jb_c1b"
                                    >Spark Evaluator enhancement</xref> - The Spark Evaluator now
                                supports Spark 2.x.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Destinations</dt>
                    <dd>
                        <ul id="ul_p5x_qty_11b">
                            <li><xref href="../Destinations/CoAPClient.dita#concept_hw5_s3n_sz">New
                                    CoAP Client destination</xref> - A destination that writes to a
                                CoAP endpoint.</li>
                            <li><xref href="../Destinations/HiveMetastore.dita#concept_gcr_z2t_zv"
                                    >Hive Metastore destination enhancements</xref>:<ul
                                    id="ul_y5w_sty_11b">
                                    <li>The destination can now <xref
                                            href="../Destinations/HiveMetastore-TableGen.dita#concept_wyr_5jv_hw"
                                            >create and update Parquet tables in Hive</xref>. </li>
                                    <li>Also, the data format property has been removed. You now
                                        specify the data format in the <xref
                                            href="../Processors/HiveMetadata.dita#concept_rz5_nft_zv"
                                            >Hive Metadata processor.</xref>
                                        <p>Since the Hive Metastore previously supported only Avro
                                            data, there is no upgrade impact. </p></li>
                                </ul></li>
                            <li dir="ltr">
                                <p><xref href="../Destinations/Kudu.dita#concept_chy_xxg_4v">Kudu
                                        destination enhancement</xref> - You can use the new
                                    Mutation Buffer Space property to set the buffer size that the
                                    Kudu client uses to write each batch.</p>
                            </li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Executors</dt>
                    <dd>
                        <ul id="ul_xl4_b5y_11b">
                            <li><xref href="../Executors/Email.dita#concept_sjs_sfp_qz">New Email
                                    executor</xref> - Use to send custom emails upon receiving an
                                event. For a case study, see <xref
                                    href="../Event_Handling/CaseStudy-SendEmail.dita#concept_t2t_lp5_xz"
                                    />.<ul id="ul_ekw_b5y_11b">
                                    <li dir="ltr">
                                        <p><xref href="../Executors/Shell.dita#concept_jsr_zpw_tz"
                                                >New Shell executor</xref> - Use to execute shell
                                            scripts upon receiving an event. </p>
                                    </li>
                                    <li dir="ltr">
                                        <p><xref
                                                href="../Executors/JDBCQuery.dita#concept_j3r_gcv_sx"
                                                >JDBC Query executor enhancement</xref> - A new
                                            Batch Commit property allows the executor to commit to
                                            the database after each batch. Previously, the executor
                                            did not call commits by default.</p>
                                        <p>For new pipelines, the property is enabled by default.
                                            For upgraded pipelines, the property is disabled to
                                            prevent changes in pipeline behavior. </p>
                                    </li>
                                    <li dir="ltr"><xref
                                            href="../Executors/Spark-Versions.dita#concept_vbm_ywb_c1b"
                                            >Spark executor enhancement</xref> - The executor now
                                        supports Spark 2.x. </li>
                                </ul></li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>REST API / Command Line Interface</dt>
                    <dd>
                        <ul id="ul_ktv_k5y_11b">
                            <li>Offset management - Both the REST API and <xref
                                    href="../Administration/CLI-Overview.dita#concept_ywx_d5x_pt"
                                    >command line interface</xref> can now retrieve the last-saved
                                offset for a pipeline and set the offset for a pipeline when it is
                                not running. Use these commands to implement pipeline failover using
                                an external storage system. Otherwise, pipeline offsets are managed
                                by Data Collector and there is no need to update the offsets.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Expression Language</dt>
                    <dd>
                        <ul id="ul_fm5_m5y_11b">
                            <li><xref
                                    href="../Expression_Language/MiscFunctions.dita#concept_ddw_ld1_1s"
                                    >vault:read enhancement</xref> - The vault:read function now
                                supports returning the value for a key nested in a map.</li>
                        </ul>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>General</dt>
                    <dd>
                        <ul id="ul_rlz_55y_11b">
                            <li><xref
                                    href="../Administration/SupportBundles.dita#concept_szj_3mw_xz"
                                    >Support bundles</xref> - You can now use Data Collector to
                                generate a support bundle. A support bundle is a ZIP file that
                                includes Data Collector logs, environment and configuration
                                information, pipeline JSON files, resource files, and pipeline
                                snapshots. <p>You upload the generated file to the StreamSets
                                    support team so that we can use the information to troubleshoot
                                    your support tickets.</p></li>
                            <li>
                                <p><xref
                                        href="../Pipeline_Configuration/SSL-TLS.dita#concept_dd1_n3f_5z"
                                        >TLS property enhancements</xref> - Stages that support
                                    SSL/TLS now provide the following enhanced set of properties
                                    that enable more specific configuration:<ul id="ul_v5g_w5y_11b">
                                        <li>Keystore and truststore type - You can now choose
                                            between Java Keystore (JKS) and PKCS-12 (p-12).
                                            Previously, Data Collector only supported JKS.</li>
                                        <li>Transport protocols - You can now specify the transport
                                            protocols that you want to allow. By default, Data
                                            Collector allows only TLSv1.2. </li>
                                        <li>Cipher suites - You can now specify the cipher suites to
                                            allow. Data Collector provides a modern set of default
                                            cipher suites. Previously, Data Collector always allowed
                                            the default cipher suites for the JRE.</li>
                                    </ul>To avoid upgrade impact, all SSL/TLS/HTTPS properties in
                                    existing pipelines are preserved during upgrade. </p>
                            </li>
                            <li>
                                <p><xref
                                        href="../Cluster_Mode/ClusterPipelines.dita#concept_hmh_kfn_1s"
                                        >Cluster mode enhancement</xref> - Cluster streaming mode
                                    now supports Spark 2.x. For information about using Spark 2.x
                                    stages with cluster mode, see <xref
                                        href="../Cluster_Mode/StageLimitations.dita#concept_pdf_r5y_fz"
                                    />.</p>
                            </li>
                            <li><xref
                                    href="../Pipeline_Design/Preconditions.dita#concept_msl_yd4_fs"
                                    >Precondition enhancement</xref> - Stages with user-defined
                                preconditions now process all preconditions before passing a record
                                to error handling. This allows error records to include all
                                precondition failures in the error message.</li>
                            <li><xref
                                    href="../Pipeline_Maintenance/ImportingPipelines.dita#task_fdf_hrr_5q"
                                    >Pipeline import</xref>/<xref
                                    href="../Pipeline_Maintenance/ExportingPipelines.dita#task_dtz_4tr_5q"
                                    >export enhancement</xref> - When you export multiple pipelines,
                                Data Collector now includes all pipelines in a single zip file. You
                                can also import multiple pipelines from a single zip file. </li>
                        </ul>
                    </dd>
                </dlentry>
            </dl></p>
 </conbody>
</concept>

<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_esz_x3d_kz">
    <title>Databricks Prerequisites</title>
    <conbody>
        <p><indexterm>Spark executor<indexterm>Databricks
                prerequisites</indexterm></indexterm><indexterm>Databricks
                    prerequisites<indexterm>Spark executor</indexterm></indexterm>Before you run a
            Spark executor pipeline that starts jobs on Databricks, perform the following tasks:<ol
                id="ol_rxc_rld_kz">
                <li>Create the job. <p>The Spark executor can start jobs based on notebooks or
                        JARs.</p></li>
                <li> Optionally configure the job to allow concurrent runs.<p>By default, Databricks
                        does not allow running multiple instances of a job at the same time. With
                        the default, if the Spark executor receives multiple events in quick
                        succession, it starts multiple instances of the job but Databricks queues
                        those instances and runs them one by one. </p><p>To enable parallel
                        processing, in Databricks, configure the job to allow concurrent runs. You
                        can configure the maximum number of concurrent runs through the Databricks
                        API with the max_concurrent_runs parameter, or through the UI using the Jobs
                        > Advanced menu and the Maximum Concurrent Runs property.</p></li>
                <li>Submit the job and note the job ID.<p>When you submit the job, Databricks
                        generates a job ID. Use the job ID when you configure the Spark
                        executor.</p></li>
            </ol></p>
    </conbody>
</concept>

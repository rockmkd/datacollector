<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_aht_p4h_c1b">
 <title>Spark Home Requirement</title>
 <conbody>
  <p><indexterm>Spark executor<indexterm>Spark home requirement</indexterm></indexterm>When running
            an application on YARN, the Spark executor requires access to the spark-submit script
            located in the Spark installation directory. </p>
        <p>By default, the Spark executor uses the directory defined in the SPARK_HOME environment
            variable on the <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            machine. The SPARK_HOME environment variable must be set before you start <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>. </p>
        <p>You can override the environment variable as needed by configuring the Custom Spark Home
            property in the executor stage properties. Use the Custom Spark Home property when the
            SPARK_HOME environment variable is not set, or when it points to a conflicting version
            of Spark. </p>
        <p>For example, if you are using a Spark 2.1 stage library for the Spark executor and
            SPARK_HOME points to a Spark 1.6 installation, use the Custom Spark Home property to
            specify the location of the Spark 2.1 spark-submit script. </p>
 </conbody>
</concept>

<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_vl3_v2f_zz">
 <title>Parquet Case Study</title>
 <conbody>
        <p><indexterm>Drift Synchronization Solution for Hive<indexterm>Parquet case
                    study</indexterm></indexterm>Let's say you have database data that you want to
            write to Parquet tables in Hive. You want to write the data to different Parquet tables
            based on the country of origin. You don't expect a lot of schema changes, but would like
            it handled automatically when it occurs. </p>
        <p>To do this, you'd start off with the JDBC Query Consumer to read data into the pipeline.
            You connect the origin to the Hive Metadata processor and configure expressions that
            define the corresponding database, table, and partition where each record should be
            written in the Parquet table. The Hive Metadata processor uses this information to
            assess records and generate the record header attributes that the data-processing
            destination uses to write the data. It also uses the information to generate metadata
            records that the Hive Metastore destination uses to create and update tables as
            needed.</p>
        <p>You connect the Hive Metadata processor data output stream to a Hadoop FS destination and
            configure it to use the information in record headers. The destination then writes each
            record using the target directory and schema information in the record header, and rolls
            files upon schema changes. And you configure the destination to generate events so it
            generates events each time it closes a file. </p>
        <p>You connect the Hive Metadata processor metadata output stream to a Hive Metastore
            destination. The destination, upon receiving the metadata record from the Hive Metadata
            processor, creates or updates Parquet tables as needed. </p>
        <p>And finally, you connect a MapReduce executor to the event stream of the Hadoop FS
            destination and configure the executor to use the Convert Avro to Parquet job available
            in the stage. So each time the executor receives an event from the Hadoop FS
            destination, it processes the closed Avro file and converts it to Parquet, writing it to
            the updated Parquet tables.</p>
        <p>Now let's take a closer look... </p>
    </conbody>
</concept>

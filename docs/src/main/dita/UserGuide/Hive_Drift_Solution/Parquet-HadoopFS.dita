<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_oyq_qbh_zz">
 <title>The Data-Processing Destination</title>
 <conbody>
        <p conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HD-CStudy-HDFS"/>
        <p id="HD-CStudy-ConnectHDFS">To write Avro files to HDFS, you connect the Hadoop FS
            destination to the data output stream of the Hive Metadata processor. </p>
        <p>First, on the General tab, enable the destination to generate events, as follows:</p>
        <p><image href="../Graphics/Parquet-HDFS-Events.png" id="image_zc3_hjh_zz" scale="65"/></p>
        <p>Now, the destination generates an event each time the destination closes an output file.
            As described in the <xref
                href="../Destinations/HadoopFS-EventRecords.dita#concept_dmx_1ln_qx">Event Record
                section</xref> of the Hadoop FS documentation, the event record includes the
            filepath and file name of the closed file. The MapReduce executor will use this
            information to convert the Avro files to Parquet.</p>
        <p
            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HD-CStudy-HDFSuseatts"/>
        <p>The Output Files tab of the destination might look something like this:</p>
        <p><image href="../Graphics/Parquet-HDFS-Output.png" id="image_ckx_kkh_zz" scale="65"/></p>
        <p><ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HD-CStudy-HDFSprocessing-ph"
            /> It closes files when it spots the roll attribute in a record header or upon reaching
            other file closure limits configured in the destination. And it generates an event each
            time it closes a file. </p>
        <p>
            <note type="tip">Data does not become available to Hive until the Avro files are
                converted to Parquet. If you want to convert data quickly, configure one or more of
                the file closure properties to ensure files roll regularly: Max Records in File, Max
                File Size, or Idle Timeout.</note>
        </p>
 </conbody>
</concept>

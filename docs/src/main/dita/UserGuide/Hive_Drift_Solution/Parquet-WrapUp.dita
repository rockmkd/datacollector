<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_enc_2wh_zz">
    <title>Processing Parquet Data</title>
    <conbody>
        <p>When the pipeline runs, the following actions occur:<ul id="ul_jqb_jfn_zz">
                <li>
                    <p>Hive Metadata processor assesses each record, using the country in the record
                        to create the output directory for the targetDirectory header attribute.</p>
                </li>
                <li>
                    <p><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HD-CStudy-HMProc-Actions"
                        /></p>
                </li>
                <li>When the Hive Metastore destination receives a metadata record, it updates the
                    Hive metastore accordingly, creating or updating a Parquet table. </li>
                <li>The Hadoop FS destination writes records to files based on the directory in the
                    targetDirectory header, closing files based on the roll header attribute and any
                    other file closure properties configured in the stage. </li>
                <li>When the Hadoop FS destination closes a file, it sends an event record to the
                    MapReduce executor, triggering the executor to kick off the Convert Avro to
                    Parquet job. The MapReduce executor does not monitor the job.</li>
                <li>After the job completes, the Parquet data becomes available to Hive.</li>
            </ul></p>
    </conbody>
</concept>

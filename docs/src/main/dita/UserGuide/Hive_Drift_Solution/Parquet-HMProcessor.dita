<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_kt2_zrf_zz">
 <title>The Hive Metadata Processor</title>
 <conbody>
        <p>Connect the JDBC Query Consumer origin to the Hive Metadata processor. When you configure
            the processor, you have a few things to consider in addition to the basic connection
            details: </p>
        <ol id="ol_kx4_fsf_zz">
            <li>Which database should the records be written to?<p>Hadoop FS will do the writing,
                    but the processor needs to know where the records should go. Let's write to the
                    Hive default database. To do that, you can leave the database property empty.
                </p></li>
            <li>What tables should the records be written to? <p>You can write all data to a single
                    table by hardcoding the Table Name property. But since you want to write the
                    data to different tables based on the country of origin, let's use an expression
                    to pull the table name from the Country field, as
                    follows:<codeblock>${record:value('/Country')}</codeblock></p></li>
            <li>What partition do you want to use?<p>Let's create a dt partition column for daily
                    partitions using datetime variables in the expression as follows:
                    <codeblock>${YYYY()}-${MM()}-${DD()}</codeblock></p></li>
            <li>How do you want to configure the precision and scale expressions for decimal fields?
                    <p>Since you have the the JDBC Query Consumer generating record header
                    attributes, you can use the default expressions in the processor:
                    <codeblock>${record:attribute(str:concat(str:concat('jdbc.', field:field()), '.scale'))}
${record:attribute(str:concat(str:concat('jdbc.', field:field()), '.precision'))}</codeblock></p><p>With
                    these expressions, the processor uses the precision and scale that is written to
                    record header attributes by the JDBC Query Consumer for each decimal field in
                    the record.</p></li>
            <li>What type of data is being processed?<p>On the Data Format tab, select the Parquet
                    data format.</p></li>
        </ol>
        <p>At this point, the pipeline looks like this:</p>
        <p><image href="../Graphics/Parquet-HiveMetadata.png" id="image_dr3_mvg_zz" scale="55"/></p>
        <p>When processing records, the Hive Metadata processor uses the configuration details to
            assess records. It generates a targetDirectory header attribute for each record using
            the country listed in the record for the table and the time the record was processed for
            the partition. </p>
        <p><ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HD-CStudy-HMProc-Actions"
            /></p>
        <p>Remember that for Parquet data, the processor <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HD-CStudy-ProcessorAvro"
            /></p>
        <p conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/Parquet-TargetDir2"/>
 </conbody>
</concept>

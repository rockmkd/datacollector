<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_gmh_b4h_zz">
 <title>The MapReduce Executor</title>
 <conbody>
  <p>To convert the Avro files generated by the Hadoop FS destination, use the Convert Avro to
            Parquet job in the MapReduce executor. Like all executors, the MapReduce executor
            performs tasks when triggered by an event. In this case, it will be the file-closure
            events generated by the Hadoop FS destination. </p>
        <p>Connect the Hadoop FS event output stream to the MapReduce executor. In addition to the
            required configuration details, select the Convert Avro to Parquet job type, then
            configure the following key job details:<ul id="ul_fdd_wth_zz">
                <li>Input Avro File - Use the default expression for this property. With the
                    default, the executor uses the directory and file name specified in the filepath
                    field of the event record. Files will be in the .avro directory, but this
                    information will be correctly noted in the event record.</li>
                <li>Keep Avro Input File - Select this if you want to keep the original Avro file.
                    By default, the executor deletes the original file after successfully converting
                    it to Parquet.</li>
                <li>Output Directory - To write the Parquet files to the original directory where
                    the data was expected - rather than the .avro directory - use the following
                        expression:<codeblock>${file:parentPath(file:parentPath(record:value('/filepath')))}</codeblock><p>The
                            <codeph>file:parentPath</codeph> function returns a file path without
                        the final separator. So this expression removes /.avro/&lt;filename> from
                        the filepath.</p><p>For example, if the original filepath is:
                        /sales/countries/nz/.avro/sdc-file, then <codeph>file:parentPath</codeph>
                        returns the following output path: /sales/countries/nz.</p></li>
            </ul></p>
        <p>Here's the pipeline and MapReduce executor configuration: </p>
        <p><image href="../Graphics/Parquet-MapReduce.png" id="image_qhp_j4n_zz" scale="70"/></p>
        <p><ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/O-EventGen-xref"
            /></p>
       
 </conbody>
</concept>
